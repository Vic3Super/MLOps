{
 "cells": [
  {
   "cell_type": "code",
   "id": "28134ee86278e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:38:50.287647Z",
     "start_time": "2025-01-27T15:38:49.621372Z"
    }
   },
   "source": [
    "import requests\n",
    "from pyarrow.table import table_to_blocks\n",
    "\n",
    "from src.load_data import load_data\n",
    "\n",
    "# Define the URL\n",
    "url = \"http://127.0.0.1:8080/predict\"\n",
    "\n",
    "data = load_data(local=True)\n",
    "clean_data = data.dropna(axis=0, how='any')\n",
    "clean_data = clean_data.iloc[0:10]\n",
    "\n",
    "# Define the JSON payload\n",
    "payload = {\n",
    "    \"columns\": clean_data.columns.tolist(),\n",
    "    \"data\": clean_data.to_numpy().tolist(),\n",
    "}\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.json())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {'predictions': [57.96398162841797, 19.628793716430664, 9.540163040161133, 47.59981918334961, 17.712688446044922, 46.04148483276367, 3.5623881816864014, 57.91485595703125, 11.787161827087402, 9.88700008392334]}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:13:12.273638Z",
     "start_time": "2025-01-27T16:13:12.268127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data_test(local=True, size=1000):\n",
    "    if local:\n",
    "        data = pd.read_csv(\"../taxi_trips.csv\", nrows=10000)\n",
    "        return data, None\n",
    "    else:\n",
    "        # Open and load the JSON file\n",
    "        try:\n",
    "            with open(\"../configs/config.json\", \"r\") as file:\n",
    "                config = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Configuration file not found at '../configs/config.json'\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON format in configuration file.\")\n",
    "        # Access specific values\n",
    "        PROJECT_ID = config[\"PROJECT_ID\"]\n",
    "        DATASET_NAME = config[\"DATASET_NAME\"]\n",
    "\n",
    "        # Initialize the BigQuery client\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        # Format: `project_id.dataset_id.table_id`\n",
    "        table_id_old = f\"{PROJECT_ID}.{DATASET_NAME}.raw_data\"\n",
    "\n",
    "        # Create a BigQuery query job\n",
    "        query = f\"SELECT * FROM `{table_id_old}`\"\n",
    "\n",
    "        # Run the query and download the results as a pandas DataFrame\n",
    "        query_job = client.query(query)  # API request\n",
    "        results_old = query_job.result().to_dataframe()\n",
    "\n",
    "        table_ref = client.dataset(DATASET_NAME).table(\"prediction\")\n",
    "        table = client.get_table(table_ref)\n",
    "        \n",
    "        print(table_ref)\n",
    "        columns = [field.name for field in table.schema if field.name not in [\"prediction\", \"timestamp\"]]\n",
    "        print(f\"columns\", columns)\n",
    "        query = f\"SELECT {', '.join([f'`{column}`' for column in columns])} FROM {table_ref}\"        \n",
    "        print(query)\n",
    "        query_job = client.query(query)\n",
    "        results_new = query_job.result().to_dataframe()\n",
    "\n",
    "        frames = [results_old, results_new]\n",
    "        result = pd.concat(frames)\n",
    "        result = result.tail(size)\n",
    "        return result, results_new"
   ],
   "id": "fd81dfdea3123d11",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:13:20.352225Z",
     "start_time": "2025-01-27T16:13:14.172046Z"
    }
   },
   "cell_type": "code",
   "source": "df, new = load_data_test(local=False)",
   "id": "b3f126f4cc245837",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbon-relic-439014-t0.chicago_taxi.prediction\n",
      "columns ['Trip ID', 'Taxi ID', 'Trip Start Timestamp', 'Trip End Timestamp', 'Trip Seconds', 'Trip Miles', 'Pickup Census Tract', 'Dropoff Census Tract', 'Pickup Community Area', 'Dropoff Community Area', 'Fare', 'Tips', 'Tolls', 'Extras', 'Trip Total', 'Payment Type', 'Company', 'Pickup Centroid Latitude', 'Pickup Centroid Longitude', 'Pickup Centroid Location', 'Dropoff Centroid Latitude', 'Dropoff Centroid Longitude', 'Dropoff Centroid Location']\n",
      "SELECT `Trip ID`, `Taxi ID`, `Trip Start Timestamp`, `Trip End Timestamp`, `Trip Seconds`, `Trip Miles`, `Pickup Census Tract`, `Dropoff Census Tract`, `Pickup Community Area`, `Dropoff Community Area`, `Fare`, `Tips`, `Tolls`, `Extras`, `Trip Total`, `Payment Type`, `Company`, `Pickup Centroid Latitude`, `Pickup Centroid Longitude`, `Pickup Centroid Location`, `Dropoff Centroid Latitude`, `Dropoff Centroid Longitude`, `Dropoff Centroid Location` FROM carbon-relic-439014-t0.chicago_taxi.prediction\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:14:12.657895Z",
     "start_time": "2025-01-27T16:14:12.654176Z"
    }
   },
   "cell_type": "code",
   "source": "df.shape",
   "id": "ff02d6fd7c867182",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 24)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:23:05.160790Z",
     "start_time": "2025-01-27T16:23:05.154572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def execute_bigquery_query(client, query):\n",
    "    \"\"\"Executes a BigQuery query and returns the results as a DataFrame.\"\"\"\n",
    "    try:\n",
    "        query_job = client.query(query)  # API request\n",
    "        return query_job.result().to_dataframe()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing query: {query}\")\n",
    "        raise e\n",
    "\n",
    "def load_data_test(local=True, size=1000, use_new_data=True):\n",
    "    \"\"\"\n",
    "    Load taxi trip data from a local CSV or BigQuery.\n",
    "\n",
    "    Args:\n",
    "        local (bool): If True, load data from a local CSV file; otherwise, fetch from BigQuery.\n",
    "        size (int): Number of rows to return from the concatenated DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the resulting DataFrame and the 'new' DataFrame if applicable.\n",
    "    \"\"\"\n",
    "    if local:\n",
    "        try:\n",
    "            data = pd.read_csv(\"../taxi_trips.csv\", nrows=10000)\n",
    "            return data\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Local CSV file not found at '../taxi_trips.csv'\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Load config\n",
    "            config_path = Path(\"../configs/config.json\")\n",
    "            if not config_path.is_file():\n",
    "                raise FileNotFoundError(\"Configuration file not found.\")\n",
    "            \n",
    "            with open(config_path, \"r\") as file:\n",
    "                config = json.load(file)\n",
    "\n",
    "            PROJECT_ID = config[\"PROJECT_ID\"]\n",
    "            DATASET_NAME = config[\"DATASET_NAME\"]\n",
    "\n",
    "            # Initialize BigQuery client\n",
    "            client = bigquery.Client()\n",
    "\n",
    "            # Query old data\n",
    "            table_id_old = f\"{PROJECT_ID}.{DATASET_NAME}.raw_data\"\n",
    "            query_old = f\"SELECT * FROM `{table_id_old}`\"\n",
    "            result_old = execute_bigquery_query(client, query_old)\n",
    "            \n",
    "            # Only return old training data \n",
    "            if not use_new_data:\n",
    "                return result_old.tail(size)\n",
    "            \n",
    "            # Query new data\n",
    "            table_ref = client.dataset(DATASET_NAME).table(\"prediction\")\n",
    "            table = client.get_table(table_ref)\n",
    "            columns = [field.name for field in table.schema if field.name not in [\"prediction\", \"timestamp\"]]\n",
    "            query_new = f\"SELECT {', '.join([f'`{column}`' for column in columns])} FROM `{PROJECT_ID}.{DATASET_NAME}.prediction`\"\n",
    "            results_new = execute_bigquery_query(client, query_new)\n",
    "\n",
    "            # Concatenate and limit size\n",
    "            frames = [result_old, results_new]\n",
    "            result = pd.concat(frames).tail(size)\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            raise e"
   ],
   "id": "3d911243dd88cd7f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:46:28.868897Z",
     "start_time": "2025-01-27T16:46:23.133948Z"
    }
   },
   "cell_type": "code",
   "source": "df = load_data_test(local=False, size=1000, use_new_data=True)\n",
   "id": "ec7b56fbb8f0310c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:46:29.367450Z",
     "start_time": "2025-01-27T16:46:29.362946Z"
    }
   },
   "cell_type": "code",
   "source": "df.shape",
   "id": "6e623aaa2f1c16cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 24)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
