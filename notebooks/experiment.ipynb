{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load Data from Bigquery",
   "id": "28c896b6c3664753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = \"carbon-relic-439014-t0\"\n",
    "dataset_id = \"chicago_taxi\"\n",
    "table_id = \"data\"\n",
    "\n",
    "# Construct the fully qualified table name\n",
    "table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Load data into a Pandas DataFrame\n",
    "query = f\"SELECT * FROM `{table_ref}` ORDER BY timestamp DESC LIMIT 150000\"\n",
    "df = client.query(query).to_dataframe()"
   ],
   "id": "e893f46f6383b25d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Analysis",
   "id": "1874d91dfaf70f3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "e571516300550b94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.describe()\n",
    "df.info()\n",
    "df.duplicated().sum() # expected 0"
   ],
   "id": "d6e31f0ea184eafd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "e60359be378f25ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop column with too many null values\n",
    "df.drop(columns=[\"pickup_census_tract\", \"dropoff_census_tract\", \"dropoff_community_area\", \"dropoff_latitude\", \"dropoff_longitude\", \"dropoff_location\"], inplace=True)"
   ],
   "id": "8a9ccb1cc4fd37df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.shape\n",
    "import seaborn as sns\n",
    "\n",
    "# visual detection of outliers\n",
    "sns.scatterplot(data=df, x=\"trip_miles\",y=\"trip_seconds\")"
   ],
   "id": "707088cf8be8d40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_test = df[df[\"trip_miles\"] < 200]\n",
    "df.shape\n",
    "sns.scatterplot(data=df_test, x=\"trip_miles\",y=\"trip_seconds\")"
   ],
   "id": "9146d0c40a61c8bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# outlier for target variable\n",
    "sns.boxplot(x=df[\"trip_total\"])"
   ],
   "id": "877a2a39ff6b2a18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Selection",
   "id": "ede4476703c91fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature engineering\n",
    "df[\"daytime\"] = df[\"trip_start_timestamp\"].dt.hour\n",
    "df['day_type'] = df['trip_start_timestamp'].dt.weekday.apply(lambda x: 'weekend' if x >= 5 else 'weekday')\n",
    "df['month'] = df['trip_start_timestamp'].dt.month\n",
    "df['day_of_week'] = df['trip_start_timestamp'].dt.dayofweek\n",
    "df['day_of_month'] = df['trip_start_timestamp'].dt.day\n",
    "df[\"avg_tips_per_taxi\"] = df.groupby(\"taxi_id\")[\"tips\"].transform(\"mean\")"
   ],
   "id": "348dca18ced61464",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get first data frame for model creation to check feature importance\n",
    "df_clean = df.drop(columns=[\"unique_key\", \"taxi_id\", \"trip_start_timestamp\", \"trip_end_timestamp\", \"timestamp\", \"fare\", \"tips\", \"trip_seconds\", \"pickup_location\"])\n",
    "df_clean.dropna(subset=[\"trip_total\"], inplace=True)"
   ],
   "id": "4f836dde2ec8e02e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_clean.head()",
   "id": "bd4b2b3807791559",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup tracking to mlflow",
   "id": "5255d960755216f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "TRACKING_URI = \"https://mlflow-service-974726646619.us-central1.run.app\"\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.autolog(disable=True)"
   ],
   "id": "ca7c43b6576581ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_feature_importance(pipeline, numerical_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Plots aggregated feature importance from a trained pipeline.\n",
    "\n",
    "    Args:\n",
    "        pipeline (Pipeline): Trained scikit-learn pipeline containing a preprocessing step and an XGBoost model.\n",
    "        numerical_cols (list): List of numerical feature names before transformation.\n",
    "        categorical_cols (list): List of categorical feature names before transformation.\n",
    "    \"\"\"\n",
    "    # Get feature importances from the model\n",
    "    feature_importances = pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "    # Retrieve transformed feature names from ColumnTransformer\n",
    "    encoder = pipeline.named_steps[\"preprocessor\"].named_transformers_[\"cat\"]\n",
    "    ohe_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "    # Combine numerical and encoded categorical feature names\n",
    "    all_feature_names = numerical_cols + list(ohe_feature_names)\n",
    "\n",
    "    # Create a DataFrame for feature importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        \"Feature\": all_feature_names,\n",
    "        \"Importance\": feature_importances\n",
    "    })\n",
    "\n",
    "    # Function to map one-hot encoded features back to their original categorical column\n",
    "    def map_original_feature(feature_name):\n",
    "        for cat_col in categorical_cols:\n",
    "            if feature_name.startswith(cat_col + \"_\"):  # One-hot encoded feature\n",
    "                return cat_col\n",
    "        return feature_name  # Numerical features remain unchanged\n",
    "\n",
    "    # Apply the mapping function\n",
    "    feature_importance_df[\"Original Feature\"] = feature_importance_df[\"Feature\"].apply(map_original_feature)\n",
    "\n",
    "    # Aggregate importance scores by original feature names\n",
    "    aggregated_importance = feature_importance_df.groupby(\"Original Feature\")[\"Importance\"].sum().reset_index()\n",
    "\n",
    "    # Sort by importance\n",
    "    aggregated_importance = aggregated_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.barh(aggregated_importance[\"Original Feature\"], aggregated_importance[\"Importance\"])\n",
    "    ax.set_xlabel(\"Feature Importance\")\n",
    "    ax.set_ylabel(\"Feature Name\")\n",
    "    ax.set_title(\"Aggregated Feature Importance (Grouped)\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    return fig"
   ],
   "id": "e4652e2eb9e03f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from mlflow.models import infer_signature\n",
    "def train_and_log_model(df, model_type, experiment_name=\"Chicago Taxi Regressor\", description = \"\"):\n",
    "    \"\"\"\n",
    "    Trains a regression model on given data and logs results to MLflow.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing features and target.\n",
    "        model_type (str): Model to use - \"xgboost\", \"random_forest\", \"lightgbm\", \"catboost\".\n",
    "        experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "        pipeline (Pipeline): Trained model pipeline.\n",
    "    \"\"\"\n",
    "    # Define categorical and numerical columns\n",
    "    categorical_cols = [\"payment_type\", \"company\", \"day_type\"]\n",
    "    numerical_cols = [\"trip_miles\", \"tolls\", \"extras\", \"daytime\", \"month\", \"day_of_week\", \"day_of_month\",\n",
    "                      \"avg_tips_per_taxi\", \"pickup_latitude\", \"pickup_longitude\", \"pickup_community_area\"]\n",
    "    target_column = \"trip_total\"\n",
    "\n",
    "    # Split data into features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define preprocessing steps with NaN handling\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Replace NaN with column mean for numerical features\n",
    "            (\"scaler\", StandardScaler())  # Scale numerical features\n",
    "        ]), numerical_cols),\n",
    "        \n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Replace NaN with most frequent value for categorical features\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))  # One-hot encode categorical variables\n",
    "        ]), categorical_cols)\n",
    "    ])\n",
    "    # Select the model\n",
    "    if model_type == \"xgboost\":\n",
    "        model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=6, random_state=42, objective=\"reg:squarederror\")\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    elif model_type == \"lightgbm\":\n",
    "        model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    elif model_type == \"catboost\":\n",
    "        model = CatBoostRegressor(iterations=1000, depth=6, learning_rate=0.1, random_seed=42, verbose=200)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Set up MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "    tags = {\n",
    "        \"env\": \"experiment\",\n",
    "        \"model_type\": model_type,\n",
    "        \"experiment_description\": \"Taxi Regressor\"\n",
    "    }\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(tags=tags, experiment_id=experiment.experiment_id, description=description) as run:\n",
    "        # Train the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # Infer model signature\n",
    "        input_example = X_test.iloc[0:100].dropna()\n",
    "        signature = infer_signature(input_example, pipeline.predict(input_example))\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        print(f\"R2: {r2}\")\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"MSE\", mse)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2\", r2)\n",
    "        mlflow.log_param(\"Training Size\", len(X_train))\n",
    "\n",
    "        fig = plot_feature_importance(pipeline, numerical_cols, categorical_cols)\n",
    "        mlflow.log_figure(fig, f'feature_importance_plot.png')\n",
    "        \n",
    "        # Log model hyperparameters\n",
    "        model_params = pipeline.named_steps[\"model\"].get_params()\n",
    "        for param, value in model_params.items():\n",
    "            mlflow.log_param(param, value)\n",
    "\n",
    "        # Log the entire pipeline as a model\n",
    "        mlflow.sklearn.log_model(pipeline, \"model_pipeline\", signature=signature, input_example=input_example)\n",
    "\n",
    "        print(\"Model and metrics logged to MLflow.\")\n",
    "\n",
    "    return pipeline"
   ],
   "id": "29693861bdc8e911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_and_log_model(df_clean, \"xgboost\",description=\"Basic model with no outlier removal\")",
   "id": "c50c00b748f6aabf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The created model performs badly and has questionable feature importance. Try a stricter outlier removal.",
   "id": "b482bf40594dbc29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stricter outlier removal\n",
    "def remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    exclude_cols = {\"pickup_latitude\", \"pickup_community_area\", \"pickup_latitude\", \"pickup_longitude\", \"tolls\"}  # Set of columns to exclude\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=[\"number\"]).columns if col not in exclude_cols]\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        original_size = len(df)\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        removed = original_size - len(df)\n",
    "    return df\n"
   ],
   "id": "c6ac305a5c2fcf7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_new = df.copy()",
   "id": "37fc7d3f58abaf9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_new[\"avg_tips_per_taxi\"] = df_new.groupby(\"taxi_id\")[\"tips\"].transform(\"mean\")\n",
    "# Remove unneeded or mostly empty values\n",
    "df_new = df_new[(df_new[\"trip_miles\"] > 0) & (df_new[\"trip_total\"] > 0)] \n",
    "df_new = df_new.drop(columns=[\"unique_key\", \"taxi_id\", \"trip_start_timestamp\", \"trip_end_timestamp\", \"timestamp\", \"fare\", \"tips\", \"trip_seconds\", \"pickup_location\",\"pickup_census_tract\", \"dropoff_location\", \"dropoff_census_tract\", \"dropoff_community_area\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "df_new.dropna(subset=[\"trip_total\"], inplace=True)"
   ],
   "id": "77c650ada2a2e811",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_new.shape",
   "id": "69fe242a8a2da709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_new = remove_outliers(df_new)",
   "id": "b1dd517c073a911a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_new.shape",
   "id": "a3ef685070d60353",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_new.head()",
   "id": "86d1b0441d0bcfc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_and_log_model(df_new, model_type=\"xgboost\", description=\"Basic model with outlier removal\")",
   "id": "c043f09b009551a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See different models for performance",
   "id": "cc348957bb98b437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_and_log_model(df_new, model_type=\"random_forest\", description=\"Basic model with outlier removal\")",
   "id": "ca7f72e37a799385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_and_log_model(df_new, model_type=\"lightgbm\", description=\"Basic model with outlier removal\")",
   "id": "d593cf05e9eb84a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_and_log_model(df_new, model_type=\"catboost\", description=\"Basic model with outlier removal\")",
   "id": "bd1ada1c423dee42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use xgboost with best performance and try different hyperparameters",
   "id": "adb0dbd3b5f7e9c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Experiment Name\n",
    "experiment_name = \"Chicago Taxi Regressor\"\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = [\"payment_type\", \"company\", \"day_type\"]\n",
    "numerical_cols = [\"trip_miles\", \"tolls\", \"extras\", \"daytime\", \"month\", \"day_of_week\", \"day_of_month\",\n",
    "                  \"avg_tips_per_taxi\", \"pickup_latitude\", \"pickup_longitude\", \"pickup_community_area\"]\n",
    "target_column = \"trip_total\"\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_new.drop(columns=[target_column])\n",
    "y = df_new[target_column]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps with NaN handling\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Replace NaN with column mean\n",
    "        (\"scaler\", StandardScaler())  # Scale numerical features\n",
    "    ]), numerical_cols),\n",
    "\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Replace NaN with most frequent value\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))  # One-hot encode categorical variables\n",
    "    ]), categorical_cols)\n",
    "])\n",
    "\n",
    "# Define Base Model\n",
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=6, \n",
    "                      random_state=42, objective=\"reg:squarederror\")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid (corrected)\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [50, 100, 200],\n",
    "    \"model__max_depth\": [3, 6, 10],  # Integer values only\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"model__subsample\": [0.8, 1.0]  # Controls sample ratio for training\n",
    "}\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "tags = {\n",
    "    \"env\": \"experiment\",\n",
    "    \"model_type\": \"xgboost\",\n",
    "    \"experiment_description\": \"Taxi Regressor with Hyperparameter Tuning\"\n",
    "}\n",
    "description = \"XGBoost with hyperparameter tuning\"\n",
    "\n",
    "# Start parent MLflow run\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id, tags=tags, description=description) as parent_run:\n",
    "    # Perform GridSearch with cross-validation\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring=\"r2\", n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Iterate through all tested hyperparameter sets\n",
    "    for i, (params, mean_score) in enumerate(\n",
    "            zip(grid_search.cv_results_[\"params\"], grid_search.cv_results_[\"mean_test_score\"])):\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"gridsearch_run_{i + 1}\", nested=True) as child_run:\n",
    "            # Train pipeline with specific hyperparameters\n",
    "            pipeline.set_params(**params)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "\n",
    "            # Infer model signature\n",
    "            input_example = X_test.iloc[0:100].dropna()\n",
    "            signature = infer_signature(input_example, pipeline.predict(input_example))\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"Run {i + 1} - Params: {params}\")\n",
    "            print(f\"MAE: {mae}\")\n",
    "            print(f\"MSE: {mse}\")\n",
    "            print(f\"RMSE: {rmse}\")\n",
    "            print(f\"R2: {r2}\")\n",
    "\n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"MAE\", mae)\n",
    "            mlflow.log_metric(\"MSE\", mse)\n",
    "            mlflow.log_metric(\"RMSE\", rmse)\n",
    "            mlflow.log_metric(\"R2\", r2)\n",
    "\n",
    "            # Log hyperparameters\n",
    "            for param, value in params.items():\n",
    "                mlflow.log_param(param, value)\n",
    "\n",
    "            # Log model\n",
    "            mlflow.xgboost.log_model(pipeline.named_steps[\"model\"], \"model_pipeline\")\n",
    "\n",
    "    # Log the best model found by GridSearch\n",
    "    best_params = grid_search.best_params_\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.xgboost.log_model(best_model.named_steps[\"model\"], \"best_model_pipeline\")\n",
    "\n",
    "    print(\"Best model and all hyperparameter runs logged to MLflow.\")"
   ],
   "id": "29bd0b953f378627",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
